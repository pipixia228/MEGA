# 1.相关知识
## 1.1 栅格化地图与矢量地图
| 特性     | 矢量化地图              | 栅格化地图          |
| ------ | ------------------ | -------------- |
| 表示方式   | 几何线条、曲线、点          | 像素网格或栅格单元      |
| 存储需求   | 存储关键点和拓扑关系，效率高     | 存储大量像素信息，效率低   |
| 精确性    | 几何连续，适合表示曲线和方向信息   | 受分辨率限制，曲线易被离散化 |
| 计算复杂度  | 查询和交互高效            | 计算密集，依赖栅格化加速   |
| 可解释性   | ==实例化==表示，清晰表达个体特性 | 需要进一步处理提取个体特性  |
| 动态物体处理 | 自然支持动态交互           | 需要额外步骤推断物体位置   |
## 1.2 注意力机制
### **注意力机制的核心思想**

在序列处理任务中，输入通常包含多个元素（如句子中的单词或图像中的区域），模型需要根据当前的上下文为每个元素分配不同的权重。注意力机制通过计算这些权重（称为注意力分数），使模型能够动态地聚焦于最相关的信息。

### **注意力机制的数学原理**

给定一组输入 X={x1,x2,…,xn}X = \{x_1, x_2, \dots, x_n\}X={x1​,x2​,…,xn​} 和一个查询向量 qqq，注意力机制的目标是根据 qqq 的信息，从 XXX 中提取最相关的部分。其核心公式如下：

1. **计算注意力分数**：
    
    - 对输入中的每个元素 xix_ixi​，计算它与查询 qqq 的相关性（称为注意力分数）。
    
    Score(q,xi)=q⋅xi\text{Score}(q, x_i) = q \cdot x_iScore(q,xi​)=q⋅xi​
    - 这里可以采用点积（Dot Product）、加性注意力（Additive Attention）等方法。
2. **归一化注意力分数（Softmax）**：
    
    - 将分数转换为概率分布，确保权重的和为1：
    
    αi=exp⁡(Score(q,xi))∑j=1nexp⁡(Score(q,xj))\alpha_i = \frac{\exp(\text{Score}(q, x_i))}{\sum_{j=1}^{n} \exp(\text{Score}(q, x_j))}αi​=∑j=1n​exp(Score(q,xj​))exp(Score(q,xi​))​
3. **加权求和**：
    
    - 将输入按注意力权重加权求和，得到最终的输出：
    
    z=∑i=1nαi⋅xiz = \sum_{i=1}^{n} \alpha_i \cdot x_iz=i=1∑n​αi​⋅xi​

## 1.3 代价图
### 依据感知和预测结果建立代价图
### 基于（强化）学习建立代价图
### 人为规则定义代价图
## 1.4 Transformer
### **Transformer 的基本结构**

Transformer 的架构由两部分组成：

1. **编码器（Encoder）**：将输入序列映射为高维表示。
2. **解码器（Decoder）**：基于编码器输出和目标序列生成最终结果。

在自动驾驶任务中，通常只使用编码器部分进行特征提取。

#### **编码器的核心模块**

1. **输入嵌入（Input Embedding）**：
    
    - 将输入数据（如文本、图像特征或矢量）转换为固定维度的嵌入向量。
2. **位置编码（Positional Encoding）**：
    
    - 由于 Transformer 不具有序列的顺序信息，位置编码通过引入位置嵌入（sin/cos 或 learnable embeddings）为输入提供位置信息。
3. **多头自注意力机制（Multi-Head Self-Attention, MHSA）**：
    
    - **自注意力机制**：
        
        - 衡量序列中每个位置对其他位置的重要性。
        - 计算公式： Attention(Q,K,V)=softmax(QKTdk)V\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)VAttention(Q,K,V)=softmax(dk​​QKT​)V
            - Q,K,VQ, K, VQ,K,V：分别是查询矩阵、键矩阵和值矩阵（从输入生成）。
            - dkd_kdk​：键向量的维度，用于归一化。
    - **多头注意力**：
        
        - 同时计算多个注意力头，捕获不同子空间的信息。
        - 输出通过线性层融合： MultiHead(Q,K,V)=Concat(head1,…,headh)WO\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^OMultiHead(Q,K,V)=Concat(head1​,…,headh​)WO
4. **前馈神经网络（Feed-Forward Network, FFN）**：
    
    - 每个位置的注意力输出经过两层全连接层，增强非线性特性： FFN(x)=ReLU(xW1+b1)W2+b2\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2FFN(x)=ReLU(xW1​+b1​)W2​+b2​
5. **残差连接与归一化（Residual Connection & Layer Normalization）**：
    
    - 每个子模块（如注意力和 FFN）前后都添加残差连接，并进行层归一化，稳定训练。
#### **解码器的核心模块**

1. **自注意力机制（Masked Self-Attention）**：
    
    - 解码器的注意力机制加入遮罩，防止未来的信息泄漏。
2. **编码器-解码器注意力（Encoder-Decoder Attention）**：
    
    - 解码器通过注意力机制与编码器的输出交互，从而结合上下文信息生成输出。
## 1.5 tocken输入
#### **1.5.1. 背景**

在自动驾驶中，感知模块的目标是从多传感器（如摄像头、激光雷达等）采集的原始数据中提取关键环境信息。这些信息需要转换成计算机能够理解和处理的结构化数据。

---

#### **1.5.2. 环境 token 嵌入的定义**

- **Token**：在自然语言处理（NLP）中，token 是句子或文档的最小单元（如单词或子单词）。在自动驾驶领域，环境 token 表示驾驶场景中的最小信息单元，例如道路标志、车道线、交通灯等。
- **嵌入（Embedding）**：将高维的稀疏数据（如图像像素）映射到低维的稠密向量空间，使其在深度学习模型中更易处理。

在 VADv2 中，“环境 token 嵌入”是对驾驶场景中关键元素的向量化表示，这些 token 包含地图信息、交通参与者的动态状态、交通标志和信号等。

---

#### **1.5.3. 转换过程**

论文中提到的转换过程包括以下步骤：

#### (1) **多视角图像的获取**

- 使用多台摄像头（通常为 6 个，覆盖 360° 视角）同时捕获周围环境的图像。

#### (2) **特征提取**

- 利用深度学习的编码器（通常是卷积神经网络或 Transformer）从图像中提取高层次的特征。
- 提取的特征分为以下几种类型的 token：
    - **地图 token**：包括道路边界、车道线、交叉路口等信息，帮助车辆理解道路结构。
    - **交通参与者 token**：描述周围车辆、行人等动态物体的状态（位置、速度、运动轨迹等）。
    - **交通元素 token**：包括交通信号灯、停车标志等静态或动态信号信息。
    - **图像 token**：直接从图像中提取的整体场景特征，包含上下文信息。

#### (3) **嵌入生成**

- 将提取的特征通过嵌入层（如多层感知机或嵌入表）转换为高维向量表示，每种 token 都有其独立的嵌入向量。
- 这些嵌入在高维空间中能够表达该 token 所代表信息的语义含义。

---

#### **1.5.4. 为什么需要环境 token 嵌入**

#### **(1) 信息结构化**

- 原始图像数据非常稀疏且高维，难以直接用于深度学习模型。
- 嵌入 token 是稠密且低维的特征表示，易于后续模型（如 Transformer）处理。

#### **(2) 融合多种信息**

- 将多种不同来源的信息（如地图、交通参与者、信号）整合在一个统一的表示空间中，有助于模型更好地理解场景。

#### **(3) 提升模型性能**

- 高层次的语义表示可以帮助规划和决策模块更准确地预测和生成合理的驾驶行为。
## 1.6 机器学习ML、深度学习DL、强化学习RL
| 特性             | 机器学习                      | 深度学习                   | 强化学习                     |
|------------------|-------------------------------|----------------------------|------------------------------|
| **目标**         | 学习模式、预测或分类          | 从高维数据中提取复杂模式   | 学习如何在环境中决策         |
| **核心方法**     | 传统算法（SVM、决策树等）     | 神经网络                   | 奖励和试错                  |
| **数据依赖**     | 中等（特征工程很重要）         | 高度依赖（需要大量数据）   | 通常不需要大量标注数据       |
| **应用场景**     | 推荐系统、预测                | 图像识别、语音处理         | 自动驾驶、机器人控制         |

# [[UniAD]]—>[[VAD]]—>[[VAD-v2]]对比
==待补充：
	开源端到端的具体项目梳理；
	相关知识补充：Transformer、BEV Former、网络搭建等==

| 特性                  | UniAD                             | VAD                              | VADv2                            |
|-----------------------|------------------------------------|-----------------------------------|-----------------------------------|
| **场景表示**         | BEV 表示结合多任务 Transformer 解码器 | 完全矢量化表示，实例级边界、车道及运动 | 矢量化表示 + 概率场建模规划动作分布 |
| **感知与预测模块**   | TrackFormer、MapFormer 等模块逐步推理 | 使用矢量查询学习代理和地图特征       | 通过环境编码器将图像数据转换为场景令牌 |
| **规划方法**         | 基于查询接口的注意力规划            | 矢量约束（碰撞、边界、车道方向）    | 概率规划，支持多模态采样和场景约束 |
| **计算效率**         | 高计算需求，模块分解带来额外开销    | 消除栅格化，显著减少计算开销         | 动作离散化减少推理时间，闭环表现稳定 |
**思考与总结**
1. 感知、预测、决策、规划集成到一个模型，相互之间的==区分度更小==（区别于传统基于规则的方法）；
2. 框架设计倾向于==增强可解释性==；
3. 在BEV Former基础上==融合更多传感器==，提升场景理解能力（感知，尤其是==动态场景==）；
4. ==不确定性规划==将会是一个重要的方向（基于概率？or博弈？）；
5. 模型优化：优化计算速度、模型表达能力、特殊情况兜底等；
6. 会有类似于传统方法的逻辑进行==安全兜底==；
**技术栈补充**
1. BEV Former、Transformer、注意力机制
2. 需要从头到尾理清一个开源项目